# NLP-101questions
About NLP interview 101 questions
<b>
<pre>
nlp技术范围 ：   问题描述

文本表示	：什么是词嵌入？
文本表示	：	词嵌入的实现方式
文本表示	：NLP中的文本表示方法有几种
文本表示	：NLP中文本分布式表示
神经网络	：	fasttext模型的结构，模型中的优点
神经网络	：	复杂结构模型和简单结构模型的优缺点
机器学习nlp	：	什么是贝叶斯公式
机器学习nlp	：	怎么使用朴素贝叶斯分类
机器学习nlp	：	聚类算法有几种，简单介绍几种
机器学习nlp	：	什么是降维算法
机器学习nlp	：	什么是主题模型
文本表示	：	介绍一下wordvector的实现方式
机器学习nlp：		隐马尔科夫模型中前向和后向算法，vertibe算法
机器学习nlp	：	什么是贝叶斯网络，模型主要原理
机器学习nlp	：	什么是条件随机场，模型主要原理
神经网络	：	推导softmax的求导过程
神经网络	：	神经网络激活函数有几种，各有什么优缺点
神经网络	：	时序数据选择怎么样的模型处理
nlp模型	：	lstm的模型结构及优化模型
nlp专业基础：		介绍一下混淆矩阵和常用的模型评判标准
nlp专业基础：		什么是信息熵，条件熵，联合熵
nlp专业基础	：	什么是交叉熵
nlp专业基础	：	什么是互信息
机器学习nlp	：	简单介绍一下最大熵模型算法原理
神经网络	：	简单介绍一下为什么可以选择交叉熵作为损失函数，交叉熵损失函数的求导
nlp专业基础	：	什么是perplexity？它在nlp里的位置
神经网络	：	ReLu损失函数有什么优缺点，有什么优化方法
机器学习nlp	：	 讲一下EM算法，E步和M步的具体步骤，E中的期望是什么（关于什么分布的期望）
nlp模型	：	LSTM的型的参数个数和时间复杂度
nlp模型	：	BatchNorm和LayerNorm的区别？
神经网络	：	L1正则和L2正则说一下 L1满足什么
神经网络	：	常用正则化方式 L1L2正则化的区别在哪里 应该如何选择他们
机器学习nlp：	逻辑回归怎么处理过拟合呢，需要对特征进行归一化么
nlp专业基础	：	有没有遇到过过拟合？为什么会过拟合？怎么处理过拟合？
nlp专业基础	：	拟合怎么办 欠拟合怎么办 怎么判断过拟合（训练集表现好、验证集表现不好） 标签分布不均衡怎么办 样本太少怎么办 （交叉验证）
机器学习nlp	：	你能简单区分一下，bagging 和 boosting 方法吗？
机器学习nlp	：	说一说集成学习
深度学习	：	深度学习有没有用到归一化？用到了什么归一化？归一化的操作是什么？有什么作用？
深度学习	：	数据不均匀对模型训练有什么影像，数据分布不均匀的问题怎么解决
机器学习nlp：传统的machine learning的算法了解哪些？你选择一个详细介绍一下
机器学习nlp	：	svm相比于LR或者Perceptron 优势在哪里
机器学习nlp	：	详细说下svm的每一步 函数 优化目标 最后变成什么形式 在什么条件下 为什么要用对偶问题
深度学习	：	bert的架构是什么 目标是什么 输入包括了什么 三个embedding输入是怎么综合的
深度学习	：	transformer里面每一层的主要构成有哪些 
深度学习	：	bert中MLM任务的具体策略是什么
深度学习	：	bert的mask策略
深度学习	：	BERT结构、transformer中feed forward的作用
深度学习	：	bert有什么可以改进的地方
深度学习	：	了解bert之后的大规模预训练模型吗
深度学习	：	bert中的双向注意力，bert中的嵌入怎么实现
深度学习	：	Seq2seq模型中decode和encode的差别有哪些
文本表示	：	Word2vec的两种训练目标是什么 其中skip-gram训练的loss function是什么
神经网络	：	生成式模型了解吗 和判别式模型有什么区别
神经网络	：	哪些网络结构可以作为生成式模型的结构
神经网络	：	生成序列的时候需要用的自回归结构 有了解吗
神经网络	：	怎么去判断生成的截止点？比如给定中文生成英文，什么时候知道模型应该停止了呢
深度学习	：	CNN结构pooling的作用是什么
nlp专业基础		过拟合产生的原因有哪些 解决方法有哪些
深度学习	：	bert的position embedding要加一些其他的运算方式
深度学习	：	RNN LSTM 为什么能缓解梯度消失
深度学习	：	梯度消失 梯度爆炸
神经网络	：	优化算法 sgd Adam
nlp专业基础：	auc指标 优缺点
机器学习nlp	：	LR和SVM有哪些区别 原理 损失函数
算法题：		你知道哪些排序算法
算法题：		反转链表（递归 非递归）
算法题：	topk最大（堆 递归算法 ）
算法题：		打印二叉树第k层
算法题	：	快速排序的时间复杂度 空间复杂度
机器学习nlp：	随机森林和gbdt的区别
机器学习nlp	：	随机森林需要特征的标准化么 减少方差 为什么 多个决策树比单个决策树好在哪里
深度学习	：	强化学习在什么场景下使用，
深度学习	：	强化学习不收敛的情况
机器学习nlp：最大似然估计推测交叉熵的时候 假设数据的分布是什么样的
nlp专业基础	：	过拟合的解决方式 dropout除了随机drop掉 还需要什么 训练的时候和预测的时候分别怎么做
nlp专业基础	：	准确率 召回率 f1 auc ROC
nlp专业基础	：	对特征工程 了解么
深度学习	：	textcnn为什么有效，有什么优缺点，怎么改进
神经网络	：	sigmoid和softmax，写一下交叉熵公式
深度学习	：	深度语义匹配模型
深度学习	：	注意力机制和自注意力机制，怎么实现注意力和自注意力
深度学习	：	transform模型的结构，
深度学习	：	transformer用的是哪种normalization，为什么不用BN
深度学习	：	transform自主力机制和实现原理，有什么需要改进点
nlp模型	：	无监督学习怎么计算文本相似度
nlp专业基础：	知识图谱嵌入讲一下TransE算法有什么缺点，如何改进
机器学习nlp：	 机器学习的衡量指标有什么，如何解决过拟合和欠拟合
算法题	：	 写一下单链表冒泡排序
文本表示	：	写一下项目中用到的跨语言Embedding的推导过程
深度学习	：	深度学习一阶优化和二阶优化的方法有哪些，基于动量的方法为什么能快速收敛
文本表示	：	Word2vec为什么能学习出语义相似的词语呢
神经网络	：	传统的softmax词向量模型为什么计算效率低，词嵌入应该从输入层获取还是输出层获取
机器学习nlp：GBDT了解吗?基分类器用的什么?分类时也是用的那个吗?说一下GBDT的原理
机器学习nlp	：	XGBoost相对GBDT原理上有哪些改进。
机器学习nlp	：	介绍一下极大似然估计，和最大后验的区别是什么
文本表示	：	word2vec中，负采样相比层次化softmax，有什么优缺点？层次化softmax能保证概率归一化吗？
nlp专业基础：	单词纠错怎么做
nlp专业基础	：	知识图谱表示学习和Word2vec这种词向量嵌入有什么异同呢
算法题	：如何识别标题党
算法题	：	如何进行实体消歧，如“普通老百姓”，可能是电视剧名字，也可能是人的称谓
nlp专业基础：	知识图谱实体关系抽取的技术
nlp专业基础	：	知识图谱实体连接和实体消岐
深度学习	：fasttext和textCNN说一下
算法题	：	如何判断一句话是否含有反动、暴力元素，有标注数据怎么做，无标注数据怎么做
深度学习	：	大型预训练模型除了bert以外还知道哪些
深度学习	：	GPT和bert间主要的区别在哪 双向transformer会带来什么样的好处 你知道GPT为什么不用双向模型吗 bert作为双向模型如何解决未来文本的信息泄漏问题
机器学习nlp：介绍逻辑回归，逻辑回归是一个分类算法，那么它是在回归什么呢?
文本表示	：使用Word2vec算法计算得到的词向量之间为什么能够表征词语之间的语义近似关系？怎么判断wordvector的好坏
nlp专业基础：在样本量较少的情况下如何扩充样本数量？
机器学习nlp	：	优化方法，拟牛顿法，梯度下降法，其他优化算法
机器学习nlp	：	介绍一下逻辑回归和线性回归
文本表示	：词向量发展历史：one-hot、NNLM、word2vec、glove、fasttext、ELMO、CoVE、GPT、BERT、ERNIE、ERNIE 2.0、bert-wwm、roberta、xlnet
深度学习	：	GPT/BERT 中分别是怎么用 Transformer 的？

</pre>
</b>
